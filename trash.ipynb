{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from scipy.io import wavfile, loadmat\n",
    "from scipy import signal\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all paths of files in a derectory including files from subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles\n",
    "\n",
    "listOfFiles = getListOfFiles('/home/nvme/data/vc1/audio/')\n",
    "\n",
    "listOfFiles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(listOfFiles):\n",
    "    try:\n",
    "        wavfile.read(path)\n",
    "        \n",
    "    except ValueError:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarms vs False misses plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Pfas, Pmisses)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating a cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix_loop(tensor1, tensor2):\n",
    "    sim = [[F.cosine_similarity(a, b, dim=0) for a in tensor2] for b in tensor1]\n",
    "    return torch.Tensor(sim)\n",
    "\n",
    "def cosine_similarity_matrix_vectorized(tensor1, tensor2):\n",
    "    B, D = tensor1.size()\n",
    "    dot = tensor2 @ tensor1.t()\n",
    "    norm1 = tensor1.norm(dim=1)\n",
    "    norm2 = tensor2.norm(dim=1).view(1, B).t()\n",
    "    dot /= norm1 * norm2\n",
    "    return dot.t()\n",
    "\n",
    "B = 5\n",
    "\n",
    "anchors = torch.randn(B, 4)\n",
    "positives = torch.randn(B, 4)\n",
    "\n",
    "print(anchors)\n",
    "print()\n",
    "print(positives)\n",
    "print(anchors.size(), positives.size())\n",
    "\n",
    "print(F.cosine_similarity(anchors, positives))\n",
    "\n",
    "sim = [[F.cosine_similarity(a, b, dim=0) for a in positives] for b in anchors]\n",
    "sim = torch.Tensor(sim)\n",
    "print(sim)\n",
    "loop_result = cosine_similarity_matrix_loop(anchors, positives)\n",
    "print(loop_result)\n",
    "vectorized_result = cosine_similarity_matrix_vectorized(anchors, positives)\n",
    "print(vectorized_result)\n",
    "print((loop_result - vectorized_result).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim)\n",
    "sim_sorted, sim_sorted_idx = sim.sort(dim=1, descending=True)\n",
    "print(sim_sorted)\n",
    "print(sim_sorted_idx)\n",
    "# Given a sim matrix Dij, if i=j a value corresponds to a similarity between \n",
    "# positive pairs -> we need to prevent them from getting to the negative samples\n",
    "# First, we need to remove i=j elements.\n",
    "mask = (sim_sorted_idx != torch.arange(B).repeat(1, B).view(B, B).t())\n",
    "sim_sorted_idx_rm = sim_sorted_idx[mask].view(B, B-1)\n",
    "print(sim_sorted_idx_rm)\n",
    "# select the indices for appropriately hard samples\n",
    "tau = 0.1\n",
    "idx_threshold = round(tau * (B-2))\n",
    "# only half of the batch size -> B // 2\n",
    "hnm_idxs = sim_sorted_idx_rm[B // 2:, idx_threshold]\n",
    "print(hnm_idxs)\n",
    "idx_threshold_rand = torch.from_numpy(np.random.uniform(size=(B, 1)) * (B-1)).long()\n",
    "# print(idx_threshold_rand)\n",
    "# rand_idxs = sim_sorted_idx_rm[:B // 2, idx_threshold_rand]\n",
    "rand_idxs = torch.gather(sim_sorted_idx_rm, dim=1, index=idx_threshold_rand)[:B // 2]\n",
    "print(rand_idxs)\n",
    "print(hnm_idxs.shape, rand_idxs.shape)\n",
    "print(torch.cat([rand_idxs.view(-1), hnm_idxs.view(-1)]))\n",
    "negatives = positives[torch.cat([rand_idxs.view(-1), hnm_idxs.view(-1)]), :]\n",
    "print(negatives)\n",
    "pos_n_neg = torch.cat([positives, negatives])\n",
    "anchors_n_anchors = torch.cat([anchors, anchors])\n",
    "labels = torch.cat([torch.ones(B), torch.zeros(B)])\n",
    "print(pos_n_neg)\n",
    "print(pos_n_neg.shape)\n",
    "print(anchors_n_anchors)\n",
    "print(anchors_n_anchors.shape)\n",
    "print(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove DC component and add a small dither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dc = loadmat('before_dc.mat')['sin'].reshape(-1)\n",
    "after_dc = loadmat('after_dc.mat')['sin'].reshape(-1)\n",
    "\n",
    "audio_path = 'Y8hIVOBuels_0000002.wav'\n",
    "# read\n",
    "rate, samples = wavfile.read(audio_path)\n",
    "\n",
    "print(before_dc.mean())\n",
    "print(after_dc.mean())\n",
    "print(samples.mean())\n",
    "\n",
    "signal.lfilter([1, -1], [1, -0.99], before_dc)\n",
    "\n",
    "(after_dc - signal.lfilter([1, -1], [1, -0.99], before_dc)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-1 and Top-5 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 7 # 1251 class num\n",
    "SIZE = (1, C)\n",
    "probs = torch.rand(SIZE) # net(spec)\n",
    "label = torch.randint(C, size=(1,)).type(torch.LongTensor)\n",
    "pred_top5 = probs.topk(5)[1]\n",
    "pred_top1 = probs.topk(1)[1]\n",
    "print(probs)\n",
    "print(label)\n",
    "print(pred_top5)\n",
    "print(label in pred_top5.view(5))\n",
    "print(label in pred_top1.view(1))\n",
    "print(label == pred_top5.view(5)[0])\n",
    "if label == pred_top5.view(5)[0]:\n",
    "    print('it works')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting model on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input (B, 1, 512, 298)\n",
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))       \n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        # no need for SoftMax because CrossEntropyLoss calculates it\n",
    "        if self.train:\n",
    "            return x\n",
    "        \n",
    "        else:\n",
    "            assert True == False, 'Decide what to do with SM on inference'\n",
    "    \n",
    "B = 3\n",
    "SIZE0 = (100, 1, 512, 298)\n",
    "SIZE1 = (100, 1, 512, 298)\n",
    "SIZE2 = (100, 1, 512, 298)\n",
    "model_input_size = (B, 1, 512, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input (B, 3, 32, 32)\n",
    "class VoiceNet(nn.Module):\n",
    "    \"Implementation Ref: https://github.com/kuangliu/pytorch-cifar\"\n",
    "    def __init__(self, num_classes=None):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', \n",
    "                 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "        self.features = self._make_layers(VGG16)\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "SIZE0 = (100, 3, 32, 32)\n",
    "SIZE1 = (100, 3, 32, 32)\n",
    "SIZE2 = (100, 3, 32, 32)\n",
    "model_input_size = (B, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/nvme/logs/VoxCeleb/_model_overfit_test'\n",
    "EPOCH_NUM = 30\n",
    "B = 96\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:1'\n",
    "NUM_WORKERS = 4\n",
    "EVAL_THRESHOLD = 0.5\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VoiceNet(num_classes=3)\n",
    "net.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), 1e-7, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "B = 3\n",
    "\n",
    "class0_data = torch.rand(SIZE0) - 20\n",
    "class1_data = torch.rand(SIZE1) + 20\n",
    "class2_data = torch.rand(SIZE2)\n",
    "\n",
    "labels0 = torch.zeros(100).type(torch.LongTensor)\n",
    "labels1 = torch.ones(100).type(torch.LongTensor)\n",
    "labels2 = torch.ones(100).type(torch.LongTensor) * 2\n",
    "\n",
    "dataset = torch.cat([class0_data, class1_data, class2_data])\n",
    "datalabels = torch.cat([labels0, labels1, labels2])\n",
    "\n",
    "shuffling_idxs = torch.randperm(len(dataset))\n",
    "dataset = dataset[shuffling_idxs]\n",
    "datalabels = datalabels[shuffling_idxs]\n",
    "\n",
    "for epoch_num in range(EPOCH_NUM):\n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    \n",
    "#     for iter_num, specs in tqdm(enumerate(dataset)):\n",
    "    for i in tqdm(range(len(dataset) // B)):\n",
    "        labels, specs = datalabels[i*B:i*B+B].view(B), dataset[i*B:i*B+B].view(model_input_size)\n",
    "        optimizer.zero_grad()\n",
    "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
    "        probs = net(specs)\n",
    "        loss = criterion(probs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TBoard\n",
    "        step_num = epoch_num * (len(dataset)//3) + i\n",
    "        TBoard.add_scalar('TrainLoss', loss.item(), step_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "trainset = IdentificationDataset(DATASET_PATH, train=True, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=3)\n",
    "\n",
    "testset = IdentificationDataset(DATASET_PATH, train=False, transform=transforms)\n",
    "testsetloader = torch.utils.data.DataLoader(testset, batch_size=1)\n",
    "\n",
    "for i, a in enumerate(trainsetloader, 0):\n",
    "    labels, specs = a\n",
    "    print(labels, specs)\n",
    "    if i > 2:\n",
    "        break\n",
    "\n",
    "for i, a in enumerate(testsetloader, 0):\n",
    "    labels, specs = a\n",
    "    print(labels, specs)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'Y8hIVOBuels_0000002.wav'\n",
    "# read\n",
    "rate, samples = wavfile.read(audio_path)\n",
    "\n",
    "## parameters\n",
    "window = 'hamming'\n",
    "# window width and step size\n",
    "Tw = 25\n",
    "Ts = 10\n",
    "# frame duration (samples)\n",
    "Nw = int(rate * Tw * 1e-3)\n",
    "# overlapped duration (samples)\n",
    "# 2 ** to the next pow of 2\n",
    "Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "nfft = 2 ** (Nw - 1).bit_length()\n",
    "pre_emphasis = 0.97\n",
    "\n",
    "# preemphasis filtering\n",
    "samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "\n",
    "# removes DC component of the signal and add a small dither\n",
    "samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
    "dither = np.random.uniform(-1, 1, samples.shape)\n",
    "spow = np.std(samples)\n",
    "samples = samples + 1e-6 * spow * dither\n",
    "\n",
    "# spectogram\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                mode='magnitude', return_onesided=False)\n",
    "\n",
    "spectrogram *= 1600\n",
    "\n",
    "print(spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-1, 1, samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, frequencies, spectrogram, cmap=plt.cm.jet)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.colorbar()\n",
    "print(spectrogram.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat('SPEC.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolormesh(times, frequencies, mat['SPEC'], cmap=plt.cm.jet)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.colorbar()\n",
    "print(spectrogram.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(mat['SPEC'][:, 200], spectrogram[:, 200], 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram[:, 123] / mat['SPEC'][:, 123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(spectrogram);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mat['SPEC']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought the model also uses frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, train):\n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        \n",
    "        if train:\n",
    "            phases = [1, 2]\n",
    "        \n",
    "        else:\n",
    "            phases = [3]\n",
    "            \n",
    "        mask = split['phase'].isin(phases)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        print(self.dataset.head(10))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        track_path = self.dataset[idx]\n",
    "        print(track_path)\n",
    "        \n",
    "        ## FACE\n",
    "        frames_path = os.path.join(self.path, 'video', track_path.replace('.wav', '.txt'))\n",
    "        frames_table = pd.read_table(frames_path, skiprows=6, usecols=['FRAME '])\n",
    "        mask = np.where(frames_table.values % 25 == 0)\n",
    "        # Note: only 20 per each face-track (see the asterics on the project page)\n",
    "        # frames_1fps = frames_table[mask]\n",
    "        frames_1fps = frames_table.values[mask][:20]\n",
    "        print(frames_1fps)\n",
    "        selected_frame = np.random.choice(frames_1fps)\n",
    "        print(selected_frame)\n",
    "        selected_frame_filename = '{0:07d}.jpg'.format(selected_frame)\n",
    "        # cut off filename and extention. Add selected filename\n",
    "        selected_frame_path = os.path.join(self.path, 'video', track_path[:-10], selected_frame_filename)\n",
    "        print(selected_frame_path)\n",
    "        \n",
    "        \n",
    "        # load this frame\n",
    "        \n",
    "        ## AUDIO\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "        \n",
    "        return 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
