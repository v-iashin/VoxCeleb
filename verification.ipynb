{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2spectrogram(path, segment_len=3, window='hamming', Tw=25, Ts=10, \n",
    "                    pre_emphasis=0.97, alpha=0.99, return_onesided=False):\n",
    "    # read .wav file\n",
    "    try:\n",
    "        rate, samples = wavfile.read(path)\n",
    "    \n",
    "    except ValueError:\n",
    "        print(path)\n",
    "        assert True==False, path\n",
    "    \n",
    "    ## parameters\n",
    "    # frame duration (samples)\n",
    "    Nw = int(rate * Tw * 1e-3)\n",
    "    Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "    # overlapped duration (samples)\n",
    "    # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "    nfft = 2 ** (Nw - 1).bit_length()\n",
    "\n",
    "    # preemphasis filter\n",
    "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "\n",
    "    # removes DC component of the signal and add a small dither\n",
    "    samples = signal.lfilter([1, -1], [1, -alpha], samples)\n",
    "    dither = np.random.uniform(-1, 1, samples.shape)\n",
    "    spow = np.std(samples)\n",
    "    samples = samples + 1e-6 * spow * dither\n",
    "\n",
    "    # segment selection\n",
    "    upper_bound = len(samples) - segment_len * rate\n",
    "    start = np.random.randint(0, upper_bound)\n",
    "    end = start + segment_len * rate\n",
    "    samples = samples[start:end]\n",
    "\n",
    "    # spectogram\n",
    "    _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                    mode='magnitude', return_onesided=return_onesided)\n",
    "\n",
    "    # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "    spec *= rate / 10\n",
    "    \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        split['label'] = split['path'].apply(lambda x: int(x.split('/')[0].replace('id1', '')) - 1)\n",
    "        \n",
    "        # make train/test id split (in paths class id numbering starts with 1)\n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        # subsetting ids for training\n",
    "        mask = split['label'].isin(trainid_arr)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        track_path = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "        \n",
    "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
    "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
    "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
    "        # PyTorch complains if label > num_classes. For ex, num_classes=1211\n",
    "        # label is 1250. train labels \\in [0, ..., 268, 309, ..., 1250]. (269 + 942 = 1211)\n",
    "        # therefore, we subtract 40 (# of test classes) from a label => label \\in [0, 1211]\n",
    "        if label >= 309:\n",
    "            label -= 40\n",
    "        \n",
    "        # make a spectrogram from a .wavfile\n",
    "        spec = wav2spectrogram(audio_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        return label, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \n",
    "        # (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spec.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spec.std(axis=1).reshape(512, 1)\n",
    "        spec = (spec - mu) / sigma\n",
    "\n",
    "        return spec\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        F, T = spec.shape\n",
    "        \n",
    "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
    "        spec = spec.reshape(1, F, T)\n",
    "        \n",
    "        # make the ndarray to be of a proper type (was float64)\n",
    "        spec = spec.astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.relu(self.bn7(self.fc7(x)))\n",
    "            x = self.fc8(x)\n",
    "        \n",
    "        # we use the fc7 output for Hard Negative Mining (inference)\n",
    "        else:\n",
    "            x = self.fc7(x)\n",
    "            x = F.normalize(x)\n",
    "        \n",
    "        # during training, there's no need for SoftMax because CELoss calculates it\n",
    "        return x\n",
    "    \n",
    "    # phase: [training_iden, inference_negative_mining, training_siamese, verif_test]\n",
    "    def forward(self, voice1, voice2=None, phase='train_iden'):\n",
    "        if phase in ['train_iden', 'eval_mining']:\n",
    "            return self.forward_once(voice1)\n",
    "        \n",
    "        elif phase in ['train_veri', 'eval_veri']:\n",
    "            voice1 = self.forward_once(voice1)\n",
    "            voice2 = self.forward_once(voice2)\n",
    "            return voice1, voice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/nvme/logs/VoxCeleb/_grad_test_{}'.format(time.time()) ## HERE\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
    "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
    "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
    "# B = 96\n",
    "# but when \n",
    "torch.backends.cudnn.deterministic = True\n",
    "# I can set B = 100\n",
    "B = 100\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_WORKERS = 4\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = VoiceNet(num_classes=1211)\n",
    "# net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = Compose([\n",
    "#     Normalize(),\n",
    "#     ToTensor()\n",
    "# ])\n",
    "\n",
    "# trainset = IdentificationDatasetTrain(DATASET_PATH, transform=transforms)\n",
    "# trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, \n",
    "#                                              num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_num in range(EPOCH_NUM):\n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "#     # train\n",
    "#     net.train()\n",
    "    \n",
    "#     for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
    "#         optimizer.zero_grad()\n",
    "#         labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
    "#         scores = net(specs, phase='train_iden')\n",
    "#         loss = criterion(scores, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # TBoard\n",
    "#         step_num = epoch_num * len(trainsetloader) + iter_num\n",
    "#         TBoard.add_scalar('Metrics/train_loss', loss.item(), step_num)\n",
    "#         TBoard.add_scalar('Metrics/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        \n",
    "# # when the training is finished save the model\n",
    "# torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "# TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del specs, labels, net\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_dict = torch.load(os.path.join(LOG_PATH, 'model_snapshot_1542979501.519298.txt'))\n",
    "\n",
    "# net = VoiceNet(num_classes=1211)\n",
    "# net.to(DEVICE)\n",
    "\n",
    "# model_dict = net.state_dict()\n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# net.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_dict = torch.load(os.path.join(LOG_PATH, 'model_snapshot_1542979501.519298.txt'))\n",
    "pretrained_dict = torch.load('/home/nvme/logs/VoxCeleb/verif_class/model_snapshot_1542979501.519298.txt')\n",
    "\n",
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "model_dict = net.state_dict()\n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# net.load_state_dict(model_dict)\n",
    "net.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, batch_size, device, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "        testid_arr = np.arange(270, 310) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "\n",
    "        # split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "        self.splits = np.array_split(trainid_arr, len(trainid_arr) // batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.splits)\n",
    "    \n",
    "    def cosine_sim_matrix(self, tensor1, tensor2):\n",
    "        B, D = tensor1.size()\n",
    "        dot = tensor2 @ tensor1.t()\n",
    "        norm1 = tensor1.norm(dim=1)\n",
    "        norm2 = tensor2.norm(dim=1).view(1, B).t()\n",
    "        dot /= norm1 * norm2\n",
    "        return dot.t()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        ## POSITIVE PART\n",
    "        ids = self.splits[idx]\n",
    "        # shuffle ids to make sure that every negative pair will consist of voices of \n",
    "        # different identities at each iteration.\n",
    "        ids = np.random.permutation(ids)\n",
    "        anchors = [0] * len(ids)\n",
    "        positives = [0] * len(ids)\n",
    "        \n",
    "        for i, id in enumerate(ids):\n",
    "            # folders have paths as follows:\n",
    "            # ids/tracks/segments\n",
    "            # for examples: id10254/7gWzIy6yIIk/00001.wav\n",
    "            # 265 -> id10265\n",
    "            full_id = 'id1{:04d}'.format(id)\n",
    "            # list all tracks for that id\n",
    "            track_list = os.listdir(os.path.join(self.path, 'audio', full_id))\n",
    "            # randomly select two tracks without replacement\n",
    "            track1, track2 = np.random.choice(track_list, 2, replace=False)\n",
    "            # select two voice tracks\n",
    "            track1_fullpath = os.path.join(self.path, 'audio', full_id, track1)\n",
    "            track2_fullpath = os.path.join(self.path, 'audio', full_id, track2)\n",
    "            # list all segments for each voice track\n",
    "            track1_segments = os.listdir(track1_fullpath)\n",
    "            track2_segments = os.listdir(track2_fullpath)\n",
    "            # randomly select two voice segments\n",
    "            track1_name = np.random.choice(track1_segments)\n",
    "            track2_name = np.random.choice(track2_segments)\n",
    "            # then construct full paths\n",
    "            voice1_path = os.path.join(track1_fullpath, track1_name)\n",
    "            voice2_path = os.path.join(track2_fullpath, track2_name)\n",
    "            # create spectrograms for selected .wav files\n",
    "            spec1 = wav2spectrogram(voice1_path)\n",
    "            spec2 = wav2spectrogram(voice2_path)\n",
    "            \n",
    "            # apply transformations\n",
    "            if self.transform:\n",
    "                spec1 = self.transform(spec1)\n",
    "                spec2 = self.transform(spec2)\n",
    "\n",
    "            # add to the list\n",
    "            anchors[i] = spec1\n",
    "            positives[i] = spec2\n",
    "        \n",
    "        # concatenate and add \"channel\" dimension\n",
    "        anchors = torch.cat(anchors).unsqueeze(1)\n",
    "        positives = torch.cat(positives).unsqueeze(1)\n",
    "        \n",
    "        # we need to keep spectrograms in memory in order to return them later\n",
    "        anchor_specs = anchors.clone()\n",
    "        positive_specs = positives.clone()\n",
    "        \n",
    "        # before feeding tensors into net, transfer them to a device (GPU)\n",
    "        anchors = anchors.to(self.device)\n",
    "        positives = positives.to(self.device)\n",
    "        \n",
    "        # calculate embeddings and make sure we switch phase back to training\n",
    "        self.model.eval()\n",
    "        anchors = self.model(anchors, phase='eval_mining') # B, 1024\n",
    "        positives = self.model(positives, phase='eval_mining') # --//---\n",
    "        self.model.train()\n",
    "        \n",
    "        # there is no need to keep tensors in GPU memory # TODO\n",
    "        anchors = anchors.cpu()\n",
    "        positives = positives.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        ## NEGATIVE PART\n",
    "        # calculate a cosine similarity matrix\n",
    "        sim_mat = self.cosine_sim_matrix(anchors, positives)\n",
    "        \n",
    "        sim_sorted, sim_sorted_idx = sim_mat.sort(dim=1)\n",
    "        # Given a sim matrix Sij, if i=j a value corresponds to a similarity between \n",
    "        # positive pairs -> we need to prevent them from getting to the negative samples\n",
    "        # First, we need to remove i=j elements.\n",
    "        B = len(ids)\n",
    "        mask = (sim_sorted_idx != torch.arange(B).repeat(1, B).view(B, B).t())\n",
    "        sim_sorted_idx_rm = sim_sorted_idx[mask].view(B, B-1)\n",
    "        \n",
    "        # HARD NEGATIVE MINING PART\n",
    "        # select the indices for appropriately hard samples\n",
    "        tau = 0.1\n",
    "        idx_threshold = round(tau * (B-2))\n",
    "        # only half of the batch size -> B // 2\n",
    "        hnm_idxs = sim_sorted_idx_rm[B // 2:, idx_threshold]\n",
    "        \n",
    "        # RANDOM PART\n",
    "        idx_threshold_rand = torch.from_numpy(np.random.uniform(size=(B, 1)) * (B-1)).long()\n",
    "        rand_idxs = torch.gather(sim_sorted_idx_rm, dim=1, index=idx_threshold_rand)[:B // 2]\n",
    "        negative_specs = positive_specs[torch.cat([rand_idxs.view(-1), hnm_idxs.view(-1)]), :]\n",
    "        \n",
    "        anchors_anchors_specs = torch.cat([anchor_specs, anchor_specs])\n",
    "        positives_negatives_specs = torch.cat([positive_specs, negative_specs])\n",
    "        labels = torch.cat([torch.ones(B), torch.zeros(B)])\n",
    "        \n",
    "        return labels, anchors_anchors_specs, positives_negatives_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        test_pairs_path = os.path.join(self.path, 'veri_test.txt')\n",
    "        self.dataset = pd.read_table(test_pairs_path, sep=' ', header=None)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label, voice1_path, voice2_path = self.dataset.iloc[idx]\n",
    "        \n",
    "        voice1_path_full = os.path.join(self.path, 'audio', voice1_path)\n",
    "        voice2_path_full = os.path.join(self.path, 'audio', voice2_path)\n",
    "        \n",
    "        spec1 = wav2spectrogram(voice1_path_full)\n",
    "        spec2 = wav2spectrogram(voice2_path_full)\n",
    "        \n",
    "        if self.transform:\n",
    "            spec1 = self.transform(spec1)\n",
    "            spec2 = self.transform(spec2)\n",
    "        \n",
    "        return label, spec1, spec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist = nn.PairwiseDistance()\n",
    "        \n",
    "    def forward(self, labels, anchors, counterparts):\n",
    "        dists = self.pdist(F.normalize(anchors), F.normalize(counterparts))\n",
    "        loss = torch.mean(labels * dists ** 2 + (1 - labels) * (self.margin - dists).clamp(0) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cdet_min(y_true, pred_dists, threshold_step=1e-3, Cmiss=1, Cfa=1, Ptar=0.01):\n",
    "    Cdets = []\n",
    "    Pmisses = []\n",
    "    Pfas = []\n",
    "    \n",
    "    for threshold in tqdm(np.arange(0, 2, threshold_step)):\n",
    "        y_pred = (pred_dists < threshold).astype(np.int16)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "        N = FP + TN\n",
    "        P = FN + TP\n",
    "        Pfa = FP / N\n",
    "        Pmiss = FN / P\n",
    "        Cdet = Cmiss * Pmiss * Ptar + Cfa * Pfa * (1 - Ptar)\n",
    "        Cdets.append(Cdet)\n",
    "        Pmisses.append(Pmiss)\n",
    "        Pfas.append(Pfa)\n",
    "        \n",
    "    Cdets = np.array(Cdets)\n",
    "    Pmisses = np.array(Pmisses)\n",
    "    Pfas = np.array(Pfas)\n",
    "    \n",
    "    return np.min(Cdets), Pfas, Pmisses#, Cdets\n",
    "\n",
    "\n",
    "def EER(Pfas, Pmisses):\n",
    "    \"\"\" Equal Error Rate\n",
    "    Returns an average value between closest Pfa and Pmiss. \n",
    "    For exmaple, Pfa = 0.114; Pmiss = 0.112, ERR = 0.113\"\"\"\n",
    "    \n",
    "    idx = np.abs(Pfas - Pmisses).argmin()\n",
    "    \n",
    "    return np.mean([Pfas[idx], Pmisses[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "net.fc8 = nn.Linear(net.fc8.in_features, 1024)\n",
    "net.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 30\n",
    "\n",
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# TODO: add a comment on different batch sizes\n",
    "trainset = VerificationDatasetTrain(DATASET_PATH, model=net, batch_size=B,\n",
    "                                    device=DEVICE, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=1, num_workers=0, \n",
    "                                             shuffle=True)\n",
    "\n",
    "testset = VerificationDatasetTest(DATASET_PATH, transforms)\n",
    "testsetloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=0)\n",
    "\n",
    "criterion = ContrastiveLoss(margin=0.6)\n",
    "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:24,  1.59it/s]\n",
      "37720it [12:20, 50.94it/s]\n",
      "100%|██████████| 2000/2000 [00:48<00:00, 42.17it/s]\n",
      "40it [00:24,  1.63it/s]\n",
      "3638it [01:09, 53.25it/s]"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCH_NUM):\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    \n",
    "    for iter_num, (labels, anchors, counterparts) in tqdm(enumerate(trainsetloader)):\n",
    "        anchors, counterparts = anchors.squeeze(0), counterparts.squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        labels, anchors, counterparts = labels.to(DEVICE), anchors.to(DEVICE), counterparts.to(DEVICE)\n",
    "        anchors, counterparts = net(anchors, counterparts, phase='train_veri')\n",
    "        loss = criterion(labels, anchors, counterparts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TBoard\n",
    "        step_num = epoch_num * len(trainsetloader) + iter_num\n",
    "        TBoard.add_scalar('Metrics_verification/train_loss', loss.item(), step_num)\n",
    "        TBoard.add_scalar('Metrics_verification/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        TBoard.add_scalar('Metrics_verification/conv5', net.conv5.weight.mean(), step_num)\n",
    "        TBoard.add_scalar('Metrics_verification/fc8', net.fc8.weight.mean(), step_num)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # test\n",
    "    net.eval()\n",
    "    \n",
    "    labels = []\n",
    "    pred_dists = []\n",
    "    net.eval()\n",
    "\n",
    "    for iter_num, (label, spec1, spec2) in tqdm(enumerate(testsetloader)):\n",
    "        label, spec1, spec2 = label.to(DEVICE), spec1.to(DEVICE), spec2.to(DEVICE)\n",
    "        spec1, spec2 = net(spec1, spec2, phase='eval_veri')\n",
    "        dist = F.pairwise_distance(spec1, spec2).item()\n",
    "\n",
    "        # append a prediction and label to results\n",
    "        labels.append(label.item())\n",
    "        pred_dists.append(dist)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    pred_dists = np.array(pred_dists)\n",
    "    \n",
    "    Cdetmin, Pfas, Pmisses = Cdet_min(labels, pred_dists)\n",
    "    eer = EER(Pfas, Pmisses)\n",
    "    \n",
    "    TBoard.add_scalar('Metrics_verification/Cdet_min', Cdetmin, epoch_num)\n",
    "    TBoard.add_scalar('Metrics_verification/EER', eer, epoch_num)\n",
    "    \n",
    "    \n",
    "# when the training is finished save the model\n",
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = np.array(labels)\n",
    "pred_dists_list = np.array(pred_dists_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (pred_dists_list > thresholds[16]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1, 0.05)\n",
    "pred_dists_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true=labels_list, y_pred=y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(confusion_matrix(y_true=labels_list, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, Cdets, Pmisses, Pfas = Cmin(labels_list, pred_dists_list, np.arange(0, 2, 1e-4))\n",
    "# best, Cdets, Pmisses, Pfas = Cmin(labels_list, np.random.uniform(size=len(labels_list)), np.arange(0, 2, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Pfas, Pmisses)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(Pfas - Pmisses).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([Pfas[8157], Pmisses[8157]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
